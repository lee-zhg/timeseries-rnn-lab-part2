{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter optimization of Keras RNN  with the Watson Machine learning Python client\n",
    "\n",
    "This notebook continues working with the RNN developed  in [Predicting Oil Prices Using an RNN in Watson Studio](https://github.com/djccarew/timeseries-rnn-lab-part1). It  contains  the steps and code to demonstrate support of deep learning experiments in Watson Machine Learning Service based on the  RNN developed previously.  It  introduces commands for getting data, training_definition persistence and hyper parameter optimization.\n",
    "\n",
    "This notebook is based on the example notebook\n",
    "[From keras experiment to scoring with watson-machine-learning-client](https://dataplatform.ibm.com/analytics/notebooks/v2/1c9801fc-5063-4564-a756-75e99be47cd0/view?access_token=d38aa735e323be34260be5fcf65813cea1f5f8a17a256e1d2f23796fdcd11a7d) which follows more or less the same steps with a model based on the MNIST handwriting digits dataset.\n",
    "\n",
    "## 1. Setup the environment\n",
    "\n",
    "Before starting to run the code in this notebook, you must perform the following setup tasks:\n",
    "\n",
    "\n",
    "i. Create a Watson Machine Learning Service instance and associate it with the Watson Studio project that contains this notebook. Information on how to do this is [here](https://github.com/djccarew/timeseries-rnn-lab-part2)\n",
    "\n",
    "ii. Add specific credentials to the Cloud Object Storage instance associated with the Watson Studio project that contains this notebook. Information on how to do this is [here](https://github.com/djccarew/timeseries-rnn-lab-part2)\n",
    "\n",
    "iii. Copy the credentials to a text file so that they can be easily copied to this notebook. Information on how to do this is  [here](https://github.com/djccarew/timeseries-rnn-lab-part2)\n",
    "\n",
    "\n",
    "### 1.1 Work with Cloud Object Storage(COS)\n",
    "\n",
    "Import the boto library, which allows Python developers to manage Cloud Object Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Some required imports\n",
    "import ibm_boto3\n",
    "from ibm_botocore.client import Config\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add your COS credentials.\n",
    "\n",
    "Copy the credentials  you saved to a text file during setup into the cell below. Note that the variable ```cos_credentials``` is a Python dictionary and should be defined with your credentials as follows:\n",
    "\n",
    "```\n",
    "cos_credentials = {\n",
    "  \"apikey\": \"___\",\n",
    "  \"cos_hmac_keys\": {\n",
    "    \"access_key_id\": \"___\",\n",
    "    \"secret_access_key\": \"___\"\n",
    "  },\n",
    "  \"endpoints\": \"https://cos-service.bluemix.net/endpoints\",\n",
    "  \"iam_apikey_description\": \"Auto generated apikey during resource-key operation for Instance - crn:v1:bluemix:public:cloud-object-storage:global:a/d86af7367f70fba4f306d3c19c469d89:6244216d-4578-4ac4-a6d8-baca423111f9::\",\n",
    "  \"iam_apikey_name\": \"auto-generated-apikey-5ed63735-bc55-4c4d-8cc2-8ac6b38f554d\",\n",
    "  \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Writer\",\n",
    "  \"iam_serviceid_crn\": \"crn:v1:bluemix:public:iam-identity::a/d86af7367f70fba4f306d3c19c469d89::serviceid:ServiceId-2c690700-a604-4ef3-b11b-34966debc9b2\",\n",
    "  \"resource_instance_id\": \"crn:v1:bluemix:public:cloud-object-storage:global:a/d86af7367f70fba4f306d3c19c469d89:6244216d-4578-4ac4-a6d8-baca423111f9::\"\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy and paste your Cloud Object Storage credentials here\n",
    "## Start COS credentials\n",
    "cos_credentials = {\n",
    "   \n",
    "}\n",
    "## End COS credentials\n",
    "\n",
    "api_key = cos_credentials['apikey']\n",
    "service_instance_id = cos_credentials['resource_instance_id']\n",
    "auth_endpoint = 'https://iam.bluemix.net/oidc/token'\n",
    "service_endpoint = 'https://s3-api.us-geo.objectstorage.softlayer.net'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the Cloud Object Storage (COS) client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = ibm_boto3.resource('s3',\n",
    "                         ibm_api_key_id=api_key,\n",
    "                         ibm_service_instance_id=service_instance_id,\n",
    "                         ibm_auth_endpoint=auth_endpoint,\n",
    "                         config=Config(signature_version='oauth'),\n",
    "                         endpoint_url=service_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the buckets needed to store training data and training results. \n",
    "\n",
    "**Important:** Bucket names have to be globally unique  - replace `nnnn` in the bucket names below  with the last 4 digits of your phone number or something else unique.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important: Bucket names have to be globally unique  -\n",
    "# replace nnnn in the bucket names below  with the last 4 digits of your phone number or something else unique.\n",
    "buckets = ['oilprice-rnn-data-nnnn', 'oilprice-rnn-results-nnnn']\n",
    "for bucket in buckets:\n",
    "    if not cos.Bucket(bucket) in cos.buckets.all():\n",
    "        print('Creating bucket \"{}\"...'.format(bucket))\n",
    "        try:\n",
    "            cos.create_bucket(Bucket=bucket)\n",
    "        except ibm_boto3.exceptions.ibm_botocore.client.ClientError as e:\n",
    "            print('Error: {}.'.format(e.response['Error']['Message']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should have our buckets created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(cos.buckets.all()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Downloading oil price  data and upload  it to COS bucketsÂ¶\n",
    "We will work with the weekly  oil prices for West Texas crude. Let's download the training data and upload  to the ``` oilprice-rnn-data``` bucket.\n",
    "\n",
    "Run the code in the cell below to create the ```OILPRICE_RNN_DATA``` folder and download the data  file from the github repository.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget\n",
    "import wget, os\n",
    "\n",
    "link = 'https://raw.githubusercontent.com/ibm-ai-education/timeseries-rnn-lab-part1/master/data/WCOILWTICO.csv'\n",
    "\n",
    "data_dir = 'OILPRICE_RNN_DATA'\n",
    "if not os.path.isdir(data_dir):\n",
    "    os.mkdir(data_dir)\n",
    "\n",
    "if not os.path.isfile(os.path.join(data_dir, os.path.join(link.split('/')[-1]))):\n",
    "    wget.download(link, out=data_dir)  \n",
    "    \n",
    "data_file_path = os.path.join(data_dir, os.path.join(link.split('/')[-1]))\n",
    "        \n",
    "!ls OILPRICE_RNN_DATA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the data file to the  Cloud Object Storage bucket you just created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = buckets[0]\n",
    "bucket_obj = cos.Bucket(bucket_name)\n",
    "\n",
    "for filename in os.listdir(data_dir):\n",
    "    with open(os.path.join(data_dir, filename), 'rb') as data: \n",
    "        bucket_obj.upload_file(os.path.join(data_dir, filename), filename)\n",
    "        print('{} is uploaded.'.format(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the data file was uploaded to Cloud Object Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for obj in bucket_obj.objects.all():\n",
    "    print('Object key: {}'.format(obj.key))\n",
    "    print('Object size (kb): {}'.format(obj.size/1024))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are done with Cloud Object Storage, we are ready to train our model.\n",
    "\n",
    "### 1.3 Work with the Watson Machine Learning instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import urllib3, requests, json, base64, time, os\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authenticate to the Watson Machine Learning service on IBM Cloud.\n",
    "\n",
    "**Note:** Copy the Watson Machine Learning service  credentials  you saved to a text file during setup into the cell below. Note that the variable ```wml_credentials``` is a Python dictionary and should be defined with your credentials as follows:\n",
    "\n",
    "```\n",
    "wml_credentials = {\n",
    "  \"url\": \"https://ibm-watson-ml.mybluemix.net\",\n",
    "  \"username\": \"___\",\n",
    "  \"password\": \"___\",\n",
    "  \"instance_id\": \"___\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy and paste your Cloud Object Storage credentials here\n",
    "## Start WML service credentials\n",
    "wml_credentials = {\n",
    " \n",
    "}\n",
    "## End WML service credentials\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install watson-machine-learning-client from pypi**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade watson-machine-learning-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import watson-machine-learning-client and authenticate to service instance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from watson_machine_learning_client import WatsonMachineLearningAPIClient\n",
    "client = WatsonMachineLearningAPIClient(wml_credentials)\n",
    "print(client.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training definitions\n",
    "\n",
    "For purpose of this example one Keras model definition of an RNN has been prepared.\n",
    "\n",
    "### 2.1 Save training definition\n",
    "Prepare training definition metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_definition_metadata = {\n",
    "            client.repository.DefinitionMetaNames.NAME: \"OILPRICE-RNN\",\n",
    "            client.repository.DefinitionMetaNames.FRAMEWORK_NAME: \"tensorflow\",\n",
    "            client.repository.DefinitionMetaNames.FRAMEWORK_VERSION: \"1.5\",\n",
    "            client.repository.DefinitionMetaNames.RUNTIME_NAME: \"python\",\n",
    "            client.repository.DefinitionMetaNames.RUNTIME_VERSION: \"3.6\",\n",
    "            client.repository.DefinitionMetaNames.EXECUTION_COMMAND: \"python3 oilprice_rnn.py\"\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get sample model definition content files from git (python keras script with RNN)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_filename = 'OILPRICERNN.zip'\n",
    "\n",
    "if os.path.isfile(code_filename):\n",
    "    !ls 'OILPRICERNN.zip'\n",
    "else:\n",
    "    !wget https://github.com/ibm-ai-education/timeseries-rnn-lab-part2/raw/master/model-source/OILPRICERNN.zip\n",
    "    !ls 'OILPRICERNN.zip'\n",
    "    \n",
    "model_filename = 'oilprice_rnn.tgz'\n",
    "if os.path.isfile(model_filename):\n",
    "    !ls 'oilprice_rnn.tgz'\n",
    "else:\n",
    "    !wget https://github.com/ibm-ai-education/timeseries-rnn-lab-part2/raw/master/model-source/oilprice_rnn.tgz\n",
    "    !ls 'oilprice_rnn.tgz'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Publish training definition in Watson Machine Learning repository**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "definition_details = client.repository.store_definition(code_filename, model_definition_metadata)\n",
    "\n",
    "definition_url = client.repository.get_definition_url(definition_details)\n",
    "definition_uid = client.repository.get_definition_uid(definition_details)\n",
    "print(definition_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Experiment definition\n",
    "\n",
    "### 3.1 Save experiment\n",
    "\n",
    "**Experiment configuration dictionary**\n",
    "\n",
    "Create experiment that will train models based on previously stored definitions.\n",
    "\n",
    "\n",
    "TRAINING_DATA_REFERENCE - location of traininng data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATA_REFERENCE = {\n",
    "                            \"connection\": {\n",
    "                                \"endpoint_url\": service_endpoint,\n",
    "                                \"access_key_id\": cos_credentials['cos_hmac_keys']['access_key_id'],\n",
    "                                \"secret_access_key\": cos_credentials['cos_hmac_keys']['secret_access_key']\n",
    "                            },\n",
    "                            \"source\": {\n",
    "                                \"bucket\": buckets[0],\n",
    "                            },\n",
    "                            \"type\": \"s3\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "TRAINING_RESULTS_REFERENCE - location of training results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_RESULTS_REFERENCE = {\n",
    "                                \"connection\": {\n",
    "                                    \"endpoint_url\": service_endpoint,\n",
    "                                    \"access_key_id\": cos_credentials['cos_hmac_keys']['access_key_id'],\n",
    "                                    \"secret_access_key\": cos_credentials['cos_hmac_keys']['secret_access_key']\n",
    "                                },\n",
    "                                \"target\": {\n",
    "                                    \"bucket\": buckets[1],\n",
    "                                },\n",
    "                                \"type\": \"s3\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the hyper parameters optimizer for your experiment. The  objective is to find the combination of hyper parameters that minimizes the *val_loss* metric (i.e. mean squared error of the test data set)  so it is indicated  as the optimizer objective. \n",
    "\n",
    "The two hyper parameters that are to be optimized are:\n",
    "\n",
    "i. **dropout_rate** - the dropout rate for the  Dropout layer in the model \n",
    "\n",
    "ii. **prev_periods** - the number of weeks of data to use to predict the next week's price . If set to 1, the input for the prediction  for *week n* will  be the price for *week n-1*. If set to 2, the input for the prediction  for *week n* will  be the prices for *week n-2* and *week n-1* and so on.\n",
    "\n",
    "**num_optimizer_steps** tells the optimizer how many models we want to train based on hyper parameter value combinations. Here 4 are used in the interestr of time. Normally you would do 6, since there are 6 possible combinations of hyper parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HPO = {\n",
    "        \"method\": {\n",
    "            \"name\": \"random\",\n",
    "            \"parameters\": [\n",
    "                client.experiments.HPOMethodParam(\"objective\", \"val_loss\"),\n",
    "                client.experiments.HPOMethodParam(\"maximize_or_minimize\", \"minimize\"),\n",
    "                client.experiments.HPOMethodParam(\"num_optimizer_steps\", 4)\n",
    "            ]\n",
    "        },\n",
    "        \"hyper_parameters\": [\n",
    "            client.experiments.HPOParameter('dropout_rate', min=0.1, max=0.5, step=0.2),\n",
    "            client.experiments.HPOParameter('prev_periods', min=1, max=2, step=1)\n",
    "        ]\n",
    "     }       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure your experiment. The experiment metadata links previously stored training definitions and provides information about compute_configuration that will be used to run the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_metadata = {\n",
    "            client.repository.ExperimentMetaNames.NAME: \"Oil Price RNN Experiment\",\n",
    "            client.repository.ExperimentMetaNames.DESCRIPTION: \"Best model for RNN oil price forecaster.\",\n",
    "            client.repository.ExperimentMetaNames.AUTHOR_EMAIL: \"yourname@youremail.com\",\n",
    "            client.repository.ExperimentMetaNames.EVALUATION_METRICS: [\"mae\"],\n",
    "            client.repository.ExperimentMetaNames.TRAINING_DATA_REFERENCE: TRAINING_DATA_REFERENCE,\n",
    "            client.repository.ExperimentMetaNames.TRAINING_RESULTS_REFERENCE: TRAINING_RESULTS_REFERENCE,\n",
    "            client.repository.ExperimentMetaNames.TRAINING_REFERENCES: [\n",
    "                        {\n",
    "                            \"name\": \"OILPRICE_RNN\",\n",
    "                            \"training_definition_url\": definition_url,\n",
    "                            \"compute_configuration\": {\"name\": \"k80x2\"},\n",
    "                            \"hyper_parameters_optimization\": HPO\n",
    "                            \n",
    "                        }],\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Store experiment in Watson Machine Learning repository**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_details = client.repository.store_experiment(meta_props=experiment_metadata)\n",
    "\n",
    "experiment_uid = client.repository.get_experiment_uid(experiment_details)\n",
    "print(experiment_uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run experiment\n",
    "\n",
    "### 4.1 Running experiments\n",
    "\n",
    "This kicks off the  experiment asynchronously. You'll have to monitor its progress below to know when it has completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_run_details = client.experiments.run(experiment_uid, asynchronous=True)\n",
    "experiment_run_uid = client.experiments.get_run_uid(experiment_run_details)\n",
    "print(experiment_run_uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The training runs will take a few minutes. Now is a good time for a break. You can check on the status of your run by running the code cell below.\n",
    "Keep running the cell below periodically  until all the training runs are in the COMPLETED state as shown below:\n",
    "```\n",
    "--------------------  ------------  ---------  --------------------  --------------------  ...\n",
    "GUID (training)       NAME          STATE      SUBMITTED             FINISHED              ...\n",
    "training-vw7UqMZiR    OILPRICE_RNN  completed  2018-04-14T13:46:10Z  2018-04-14T13:53:47Z  ...\n",
    "training-vw7UqMZiR_0  OILPRICE_RNN  completed  2018-04-14T13:47:22Z  -                     ...\n",
    "                                                                                           ...\n",
    "training-vw7UqMZiR_1  OILPRICE_RNN  completed  2018-04-14T13:47:22Z  -                     ...\n",
    "                                                                                           ...\n",
    "training-vw7UqMZiR_2  OILPRICE_RNN  completed  2018-04-14T13:47:22Z  -                     ...\n",
    "                                                                                           ...\n",
    "training-vw7UqMZiR_3  OILPRICE_RNN  completed  2018-04-14T13:47:22Z  -                     ...                                                                         \n",
    "                                                                                           ...\n",
    "--------------------  ------------  ---------  --------------------  --------------------  ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep running this cell periodically  until all the training runs are in the COMPLETED state as illustrated above:\n",
    "client.experiments.list_training_runs(experiment_run_uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the experiment is completed, the next order of business is to find out which training run performed the best and what are the corresponding hyper parameters for that run.\n",
    "\n",
    "All that infomation is available via the ```client.experiments.get_run_details(...)``` call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_run_details = client.experiments.get_run_details(experiment_run_uid)\n",
    "# print(json.dumps(experiment_run_details, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Assessing the results\n",
    "\n",
    "Rather than navigate through the reams of information about the experiment, lets put the  stuff we're interested  in into a Data Frame so it's easier to work with. We'll get the results of each training run and the hyper parameters values used.\n",
    "\n",
    "**Note:** In practice you could export this to a CSV file or stick it in a database so you can peruse it later at your leisure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "rows_list = []\n",
    "for m in experiment_run_details['entity']['training_statuses']:\n",
    "    if len (m['metrics']) > 0:\n",
    "        last_metric = m['metrics'][ len (m['metrics']) - 1]\n",
    "        for h in m['hyper_parameters']:\n",
    "            if h['name'] == 'dropout_rate':\n",
    "               dropout_rate = h['double_value']\n",
    "            else:\n",
    "               prev_periods = h['int_value']\n",
    "        for v in last_metric['values']:\n",
    "            if v['name'] == 'loss' or v['name'] == 'val_loss':\n",
    "               val_loss = v['value']\n",
    "            else:\n",
    "               val_mae = v['value']\n",
    "        one_row = [m['training_guid'],  last_metric['phase'], val_mae, val_loss, dropout_rate,  prev_periods]\n",
    "        rows_list.append(one_row)\n",
    "            \n",
    "metrics_df = pd.DataFrame(rows_list,columns=['GUID', 'PHASE', 'MAE', 'VAL LOSS', 'DROPOUT', 'PREV PERIODS'])\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the winner is ?????\n",
    "\n",
    "Look for the run that had the lowest validation loss (ie mean squared error) on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run_df = metrics_df.nsmallest(1, 'VAL LOSS')\n",
    "best_run_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create online deployment\n",
    "\n",
    "You can deploy the model as a web service (online) using the Watson Machine Learning service API\n",
    "\n",
    "### 5.1 Store trained model\n",
    "\n",
    "Save the model in the Watson Machine Learning repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    client.repository.ModelMetaNames.NAME: \"Oil Price RNN Model\",\n",
    "    client.repository.ModelMetaNames.FRAMEWORK_NAME: \"tensorflow\",\n",
    "    client.repository.ModelMetaNames.FRAMEWORK_VERSION: \"1.11\",\n",
    "    client.repository.ModelMetaNames.FRAMEWORK_LIBRARIES: [{'name':'keras', 'version': '2.2.4'}]\n",
    "}\n",
    "published_model = client.repository.store_model( model=model_filename, meta_props=metadata )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Create online deployment from stored model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the stored model as an online web service deployment\n",
    "published_model_uid = client.repository.get_model_uid(published_model)\n",
    "deployment_details = client.deployments.create(published_model_uid, name=\"Oil Price RNN Deployment\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract scoring endpoint from deployment details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_url = client.deployments.get_scoring_url(deployment_details)\n",
    "print(scoring_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Scoring\n",
    "\n",
    "Prepare sample scoring data to score the deployed model\n",
    "\n",
    "We'll use the last week(s) of data in the dataset as input to predict the price of the week of 4/6/2018 (the last week covered by the data set is 3/30/2018).\n",
    "\n",
    "The input node of our model expects data with shape (1, n) where n is the value of the hyper parameter prev_periods so we need to make sure our data to be scored is shaped accordingly\n",
    "\n",
    "Read in original price data and scale it between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "prices_df = pd.read_csv(data_file_path, index_col='DATE')\n",
    "\n",
    "# Create a scaled version of the data with oil prices normalized between 0 and 1\n",
    "values = prices_df['WCOILWTICO'].values.reshape(-1,1)\n",
    "values = values.astype('float32')\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get last week of available data to predict the following week "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab last week of scaled data and reshape into shape expected by model \n",
    "scaled_last_prices = scaled[len(scaled) - 1:len(scaled),:]\n",
    "scaled_last_prices = np.reshape(scaled_last_prices, (1, 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare scoring payload and score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring_data = {'values': [scaled_last_prices.tolist()]}\n",
    "predictions = client.deployments.score(scoring_url, scoring_data)\n",
    "scaled_prediction = predictions['values'][0][0][0]\n",
    "print(\"Scaled prediction: \" + str(scaled_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert scaled prediction to dollars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform scaled prediction back to a USD price\n",
    "next_price_inverse = scaler.inverse_transform(np.full((1,1),scaled_prediction).reshape(-1, 1))\n",
    "print(\"Prediction in USD: \" + str(next_price_inverse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Summary and next steps\n",
    "\n",
    "You successfully completed this notebook! You learned how to use the watson-machine-learning-client to run experiments. Check out the [Online Documentation](https://dataplatform.ibm.com/docs/content/analyze-data/wml-setup.html) for more samples, tutorials, documentation, how-tos, and blog posts.\n",
    "\n",
    "Copyright Â© 2017, 2018 IBM. This notebook and its source code are released under the terms of the MIT License"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
